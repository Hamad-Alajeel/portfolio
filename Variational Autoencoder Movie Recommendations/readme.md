# Introduction to Variational Autoencoders:
---
Variational Autoencoders are models which learn latent space representations in lower dimensions of data samples in higher dimensional spaces, by implementing two steps in the trianing process: Encoding and Decoding. The ultimate goal of a Variational Autoencoder is to learn the joint distribution of a given dataset $p(x,z)$ in order to be able to reconstruct input data samples from a given dataset. However, in order to learn this joint distrubution, knowledge of the posterior $p(z|x)$ is required. In most cases this is intractable to calculate which is a reason for why one wouldn't be able to apply an algorithm like Expectation Maximization to learn this joint distribution. Therefore, the variational autoencoder takes this into account by maximizing a lower bound on the log likelihood of the conditional probability which uses a function that approximates the posterior. This function is learnt at the encoder stage of the variational autoencoder, and is parameterized by $\phi$: $q_{\phi}(z|x)$. The variational autoencoder samples values for z after the decoder stage, and uses that to compute the conditional probability $p_{\theta}(x|z)$ by feedforwarding through the decoder stage. The output of this model, then represents an attempt at reconstructing the input data given learnt conditional and posterior distributions for this dataset.



